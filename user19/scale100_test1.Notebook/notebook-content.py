# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "ded7bbd0-2c15-4c80-94d9-178f9f31e524",
# META       "default_lakehouse_name": "lh_tpch19_100",
# META       "default_lakehouse_workspace_id": "3c6cce90-d67d-4708-9c1f-574ad7d54f9f",
# META       "known_lakehouses": [
# META         {
# META           "id": "ded7bbd0-2c15-4c80-94d9-178f9f31e524"
# META         }
# META       ]
# META     }
# META   }
# META }

# CELL ********************

# Welcome to your new notebook
# Type here in the cell editor to add code!


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

df = spark.sql("SELECT * FROM lh_tpch19_100.lineitem")
display(df)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# MAGIC %%sql
# MAGIC 
# MAGIC SELECT count(distinct partkey), count(distinct shipdate), count(*) FROM lh_tpch19_100.lineitem

# METADATA ********************

# META {
# META   "language": "sparksql",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# MAGIC %%sql
# MAGIC 
# MAGIC select partkey, count(distinct shipdate)
# MAGIC from lh_tpch19_100.lineitem
# MAGIC group by 1
# MAGIC limit 20

# METADATA ********************

# META {
# META   "language": "sparksql",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Code generated by Data Wrangler for PySpark DataFrame

from pyspark.sql import types as T

def clean_data(df):
    # Change column type to datetime64[ns] for column: 'shipdate'
    df = df.withColumn('shipdate', df['shipdate'].cast(T.TimestampType()))
    # Sort by columns: 'partkey' (ascending), 'shipdate' (descending)
    df = df.sort(df['partkey'].asc(), df['shipdate'].desc())
    # Drop duplicate rows in column: 'partkey'
    df = df.dropDuplicates(['partkey'])
    return df

df_clean1 = clean_data(df)

display(df_clean1)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

display(df_clean1)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

display(df_clean1.select("partkey").count())

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

df_clean1 = clean_data(df)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

df_clean2 = clean_data(df)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Code generated by Data Wrangler for PySpark DataFrame

from pyspark.sql import types as T

def clean_data(df):
    # Change column type to datetime64[ns] for column: 'shipdate'
    df = df.withColumn('shipdate', df['shipdate'].cast(T.TimestampType()))
    # Sort by columns: 'partkey' (ascending), 'shipdate' (descending)
    df = df.sort(df['partkey'].asc(), df['shipdate'].desc())
    # Drop duplicate rows in column: 'partkey'
    df = df.dropDuplicates(['partkey'])
    return df

df_clean2 = clean_data(df)

display(df_clean2)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

display(df_clean2.select("partkey").count())

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

display(df_clean2.select("partkey").distinct().count())

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

df_clean4 = clean_data(df)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

display(df_clean4)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Code generated by Data Wrangler for PySpark DataFrame

from pyspark.sql import functions as F
from pyspark.sql import types as T

def clean_data(df_clean4):
    # Change column type to datetime64[ns] for column: 'receiptdate'
    df_clean4 = df_clean4.withColumn('receiptdate', df_clean4['receiptdate'].cast(T.TimestampType()))
    # Convert text to lowercase in column: 'shipinstruct'
    df_clean4 = df_clean4.withColumn('shipinstruct', F.lower(F.col('shipinstruct')))
    return df_clean4

df_clean4_clean = clean_data(df_clean4)
display(df_clean4_clean)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Code generated by Data Wrangler for PySpark DataFrame

def clean_data(df_clean4_clean):
    # Sort by column: 'orderkey' (descending)
    df_clean4_clean = df_clean4_clean.sort(df_clean4_clean['orderkey'].desc())
    # Sort by column: 'suppkey' (descending)
    df_clean4_clean = df_clean4_clean.sort(df_clean4_clean['suppkey'].desc())
    # Sort by column: 'quantity' (descending)
    df_clean4_clean = df_clean4_clean.sort(df_clean4_clean['quantity'].desc())
    # Sort by column: 'partkey' (ascending)
    df_clean4_clean = df_clean4_clean.sort(df_clean4_clean['partkey'].asc())
    return df_clean4_clean

df_clean4_clean_1 = clean_data(df_clean4_clean)
display(df_clean4_clean_1)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
